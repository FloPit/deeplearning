{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Remaining Useful Life Estimation in Prognostics Using Deep Convolution Neural Networks\" by Xiang Li et al.\n",
    "\n",
    "This document reproduces the implementation of a Deep LSTM Network  by Xiang Li et al. applied to the NASA \"CMAPSS\" dataset. This implementation has been done in Keras.\n",
    "\n",
    "Copyright (c) by Manuel Arias, Christian Schneebeli and Lukas R. Peter 2017-12-01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping into the problem, let's run the cell below to load the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras import optimizers\n",
    "from IPython.display import SVG, clear_output\n",
    "from keras.utils import plot_model\n",
    "from os import path\n",
    "\n",
    "import keras.callbacks\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "get_ipython().magic(u'matplotlib inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1  \"CMAPSS\" Dataset\n",
    "\n",
    "The C-MAPSS dataset FD001 from NASA was used to evaluate our model. This dataset provides degradation trajectories of 100 turbofan engines with unknown and different initial health condition for one operative set-point and one failure mode. The data were synthetically generated with the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dynamical model. The training data contains multivariate sensors readings of the complete run-to-failure trajectories. Therefore, the records stop at the cycle/time the engine failed. For the test set truncated time-series of various lengths prior to failure are provided for 100 engines. A total number of 20k and 13k cycles are available for the training and test set respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details of the \"CMAPSS\" dataset**\n",
    "- Training: 17731 inputs from 100 engine trajectories. It uses a sliding time window of 30 time stamps. \n",
    "- Test: 100 points from 100 engine trajectories. It takes the last available 30 time stamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1  Data Pre-processing\n",
    "\n",
    "As proposed in Li, we follow a three-step process to create the input sequences to LSTM and CNN models. Since the FD001 dataset is limited to one operative condition, 10 out of the 24 sensors show constant values. Therefore, we first dropped these values and we normalized the other 14 sensors by min/max-normalization to a range $[-1, 1]$. Second, the original dataset was processed with a sliding time window approach of size $N_f = 30$ and stride of 1. The sliding window means that the first input sample to the network takes measurements from cycles 1-30, the second 2-31, the third 3-32, and so on for each unit of the fleet. The RUL label for a sample is then simply the total number of cycles of the engine is able to operate minus the cycle where the window ends. As discussed in Li, we use a window size of 30 cycles/times since the smallest test sample consists of 30 cycles. This approach provides 17731 training samples. Lastly, the maximum horizon of prediction for RUL i.e $R_{early}$ was limited to 125 cycles following the standard procedure adopted by other researchers Li, Malhotra. This has a noticeable impact on the model accuracy and makes our models more stable. From a practical point of view, it implies that we are not interested in prediction RUL further away than 125 cycles ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time window details**\n",
    "- $N_{tw} = 30$ \n",
    "- Stride = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_tw     = 30                                                               # Time Window (N_tw)\n",
    "R_early  = 125                                                              # Max RUL in training set\n",
    "stride   = 1\n",
    "sel      = np.array([6, 7, 8, 11, 12, 13, 15, 16, 17, 18, 19, 21, 24, 25])  # Index of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(data, N_tw = 30, stride = 1):\n",
    "    N_en = np.unique(data[:,0]).shape[0]                            # Number of engines (N_en)\n",
    "    m = 0\n",
    "    for i in range(N_en):\n",
    "        n_H   = data[data[:,0] == i+1,0].shape[0]\n",
    "        N_sw  = int((n_H- N_tw) / stride + 1)                       # Number of sliding windows for engine 'i' \n",
    "        for h in range(N_sw):\n",
    "            m = m + 1    \n",
    "    return m, N_en            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(N_tw, stride, sel, R_early):\n",
    "    # Load training data\n",
    "    basepath        = path.dirname(os.getcwd()) \n",
    "    train_set       = np.loadtxt(\"train_FD001.txt\")  # Training set\n",
    "    train_set_x_org = train_set[:,sel]                              # Training set input space (x)    \n",
    "    train_set_c     = train_set[:,np.array([1])]                    # Training set cycles (c)\n",
    "    \n",
    "    # Normalize the data\n",
    "    ub = train_set_x_org.max(0)\n",
    "    lb = train_set_x_org.min(0)    \n",
    "    train_set_x = 2 * (train_set_x_org - lb) / (ub - lb) - 1   \n",
    "   \n",
    "    N_ft    = sel.shape[0]                                           # Nunber of features (N_ft)\n",
    "    m, N_en = sliding_window(train_set, N_tw, stride)                # Number of training data & engines\n",
    "    \n",
    "    train_x = np.empty((m, N_tw, N_ft), float)\n",
    "    train_y = np.empty((m), float)\n",
    "    \n",
    "    k = 0\n",
    "    for i in range(N_en):\n",
    "        idx       = train_set[:,0] == i+1                            # Index for engine number 'i'\n",
    "        train_i_x = train_set_x[idx,:]                               # Engine 'i' training  data\n",
    "        train_i_c = train_set_c[idx]                                 # Engine 'i' cycles (c)\n",
    "        train_i_y = train_i_c[-1] - train_i_c                        # RUL: Remaining Useful Lifetime for engine 'i'\n",
    "        train_i_y[train_i_y > R_early] = R_early                     # R_early = 125\n",
    "        N_sw      = int((train_i_x.shape[0] - N_tw) / stride + 1)    # Number of sliding windows for engine 'i' \n",
    "        for h in range(N_sw):\n",
    "            k = k + 1\n",
    "            vert_start = h * stride\n",
    "            vert_end   = h * stride + N_tw\n",
    "            train_i_x_slice = train_i_x[vert_start:vert_end,:]       # Training input data for engine 'i' on time window 'h'\n",
    "            train_i_y_slice = train_i_y[vert_end-1,:]                # Training output data for engine 'i' on time window 'h'\n",
    "            train_i_x_slice.shape = (N_tw, N_ft)                  # Reshape training set input (N_tw, N_ft, 1)\n",
    "            train_i_y_slice.shape = (1)                           # Reshape training set output (1, 1)\n",
    "            train_x[k-1,:] = train_i_x_slice\n",
    "            train_y[k-1] = train_i_y_slice\n",
    "     \n",
    "    # Load test data\n",
    "    test_set       = np.loadtxt(\"test_FD001.txt\")\n",
    "    test_set_x_org = test_set[:,sel]                                 # Test set input space (x)\n",
    "    test_set_c     = test_set[:,np.array([1])]                       # Test set cycles (c)\n",
    "    test_y         = np.loadtxt( \"RUL_FD001.txt\")    # Test set RUL (c)\n",
    "    test_y.shape   = (test_y.shape[0], 1)\n",
    "    \n",
    "    # Normalize the data\n",
    "    test_set_x = 2 * (test_set_x_org - lb) / (ub - lb) - 1   \n",
    "    \n",
    "    m_ts, N_en_ts = sliding_window(test_set, N_tw, stride)           # Number of training data & engines\n",
    "    \n",
    "    test_x = np.empty((N_en_ts, N_tw, N_ft), float)\n",
    "    \n",
    "    k = 0\n",
    "    for ii in range(N_en_ts):\n",
    "        engine         = test_set[:,0] == ii+1                       # Index for engine number 'i'\n",
    "        test_i_x       = test_set_x[engine,:]                        # Engine 'i' test  data\n",
    "        test_i_x_slice = test_i_x[-N_tw:]                          # Training input data for engine 'i' on time window 'h'\n",
    "        test_i_x_slice.shape = (N_tw, N_ft)                       # Reshape training set input (N_tw, N_ft, 1)\n",
    "        test_x[ii,:] = test_i_x_slice\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 17731\n",
      "number of test examples = 100\n",
      "X_train shape: (17731, 30, 14)\n",
      "Y_train shape: (17731,)\n",
      "X_test shape: (100, 30, 14)\n",
      "Y_test shape: (100, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_dataset(N_tw, stride, sel, R_early)\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2  Network Structure\n",
    "\n",
    "The proposed deep learning method consists of two sub-structures, i.e. 5-layer LSTM networks and fully-connected layer for regression.\n",
    "\n",
    "First, the input data sample is prepared in 2-dimensional (2D) format. The dimension of the input is $N_{tw} × N_{ft}$, where $N_{tw}$ denotes the time sequence dimension and $N_{ft}$ is the number of selected features (i.e. number of sensor measurements).\n",
    "\n",
    "Next, 5 identical LSTM layers are stacked in the network for feature extraction. Each layer consists of 42 neurons and the activation function is relu.\n",
    "\n",
    "Afterwards, the 2-dimensional feature map is connected with a fully-connected layer. Finally, one neuron is attached at the end of the proposed network for $RUL$ estimation.\n",
    "\n",
    "**Model Hyperparameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Activation\n",
    "activ = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3  Model in Keras\n",
    "Keras uses a different convention with variable names than TensorFlow. In particular, rather than creating and assigning a new variable on each step of forward propagation such as X, Z1, A1, Z2, A2, etc. for the computations for the different layers, in Keras code each line above just reassigns X to a new value using X = .... In other words, during each step of forward propagation, we are just writing the latest value in the commputation into the same variable X. The only exception was X_input, which we kept separate and did not overwrite, since we needed it at the end to create the Keras model instance (model = Model(inputs = X_input, ...) above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_2(input_shape,activ, layer):\n",
    "    \"\"\"\n",
    "    Implementation of the 1D_CNN model.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "      \n",
    "    # Define the input placeholder as a tensor with shape input_shape    \n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = LSTM(layer,return_sequences=True, activation=activ)(X_input)\n",
    "    X = LSTM(layer,return_sequences=True,activation=activ)(X)\n",
    "    X = LSTM(layer,return_sequences=True,activation=activ)(X)\n",
    "    X = LSTM(layer,return_sequences=True,activation=activ)(X)\n",
    "    X = LSTM(layer)(X)   \n",
    "\n",
    "    X = Dense(100, activation='relu', name='fc')(X)\n",
    "    X = Dense(1, name='RUL')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='CNN_2d')    \n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we feed the model hyperparameters to the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model\n",
    "LSTM_2d = LSTM_2(X_train.shape[1:], activ, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 30, 14)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 30, 42)            9576      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 42)            14280     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 42)            14280     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 42)            14280     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 42)                14280     \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 100)               4300      \n",
      "_________________________________________________________________\n",
      "RUL (Dense)                  (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 71,097\n",
      "Trainable params: 71,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM_2d.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Model Training\n",
    "\n",
    "Its configuration is determined including the number of hidden layers and length etc. The LSTM takes as the inputs the normalized training data, and the labeled RUL values for the training samples are used as the target outputs of the network\n",
    "\n",
    "The optimization of the network's weights was carried out with mini-batch stochastic gradient descent (SGD) and with the Adam algorithm. Xavier initializer is used for the weight initializations. The learning rate was set to 0.001 and was kept constant for the whole 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_2d.compile(optimizer = \"Adam\", loss = \"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Learning rate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "print('Learning Rate: ' + str(K.get_value(LSTM_2d.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an updatable plot to track training evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updatable plot\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []        \n",
    "        self.fig = plt.figure()        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, np.sqrt(self.losses), label=\"loss\")\n",
    "        plt.plot(self.x, np.sqrt(self.val_losses), label=\"val_loss\")\n",
    "        plt.ylabel('loss - RMSE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train','test'], loc='upper left')\n",
    "        plt.title('model loss = ' + str(min(np.sqrt(self.val_losses))))\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_2d.fit(X_train, Y_train, epochs = 50, batch_size = 512, validation_data = (X_test, Y_test), callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Learning rate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "print('Learning Rate: ' + str(K.get_value(LSTM_2d.optimizer.lr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model (architecture, weights, ...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_2d.save('5-layer-LSTM.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Performance Metrics\n",
    "Three metrics were considered to compare our results: time of training, RMSE and the scoring function proposed in NASAdata. The scoring function $s$ is defined as follows.\n",
    "\n",
    "\\begin{align} \\label{eq:someequation}\n",
    " s &= \\sum_{i=1}^{N_{s}} exp(\\alpha|\\Delta_i|) \n",
    "\\end{align}\n",
    "Here $N_s$ denotes the total number of data samples, $\\Delta_i$ is the difference between the estimated and real RUL and $\\alpha$ is $\\frac{1}{13}$ if we under-estimate and $\\frac{1}{10}$ otherwise. Thus, this metric penalizes over-estimation more than under-estimation.\n",
    "\n",
    "We use the standard definition of the root-mean-square error (RMSE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_cal(y_hat, Y_test):\n",
    "    d   = y_hat - Y_test\n",
    "    tmp = np.zeros(d.shape[0])\n",
    "    for i in range(d.shape[0]):\n",
    "        if d[i,0] >= 0:\n",
    "           tmp[i] = np.exp( d[i,0]/10) - 1\n",
    "        else:\n",
    "           tmp[i] = np.exp(-d[i,0]/13) - 1\n",
    "    return tmp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular metric to evaluate the effectiveness of the proposed method is Root Mean Square Error (RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.2 Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 31s 2ms/step\n",
      "()\n",
      "Test  MSE = 156.917735729\n",
      "Test RMSE = 12.5266809542\n"
     ]
    }
   ],
   "source": [
    "preds = LSTM_2d.evaluate(x = X_train, y = Y_train)\n",
    "print()\n",
    "print (\"Test  MSE = \" + str(preds))\n",
    "print (\"Test RMSE = \" + str(np.sqrt(preds)))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_tr   = LSTM_2d.predict(x = X_train)\n",
    "#score_i_tr = score_cal(y_hat_tr, Y_train)\n",
    "#score_tr   = print(\"Score = \" + str(sum(score_i_tr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tr = y_hat_tr - Y_train\n",
    "plt.hist(d_tr, bins='auto')  \n",
    "plt.title('Error distribution - Training Set')\n",
    "plt.ylabel('f')\n",
    "plt.xlabel(\"Error: $RUL_{hat}$ - RUL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.3 Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat   = LSTM_2d.predict(x = X_test)\n",
    "#score_i = score_cal(y_hat, Y_test)\n",
    "#score   = print(\"Score = \" + str(sum(score_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/step\n",
      "()\n",
      "MSE = 191.373088989\n",
      "RMSE = 13.8337662619\n"
     ]
    }
   ],
   "source": [
    "preds = LSTM_2d.evaluate(x = X_test, y = Y_test)\n",
    "print()\n",
    "print (\"MSE = \" + str(preds))\n",
    "print (\"RMSE = \" + str(np.sqrt(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = y_hat - Y_test\n",
    "plt.hist(d, bins='auto')  \n",
    "plt.title('Error distribution - Test Set')\n",
    "plt.ylabel('f')\n",
    "plt.xlabel(\"Error: $RUL_{hat}$ - RUL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x     = range(0,100)\n",
    "y_ts  = np.sort(Y_test[:,0])\n",
    "idx   = np.argsort(Y_test[:,0])\n",
    "y_tr  = y_hat[idx,0]\n",
    "plt.plot(x, y_tr, 'bo-', x, y_ts, 'ro-')\n",
    "plt.title('RUL vs. engine #')\n",
    "plt.ylabel('RUL')\n",
    "plt.xlabel('engine #')\n",
    "plt.legend(['Prediction', 'Target'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_test, y_hat, 'bo')\n",
    "plt.plot(Y_test,Y_test, 'r-')\n",
    "plt.plot(Y_test,Y_test+20, 'r--')\n",
    "plt.plot(Y_test,Y_test-20, 'r--')\n",
    "plt.title('RUL vs. RUL #')\n",
    "plt.ylabel('RUL Estimated')\n",
    "plt.xlabel('RUL True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/step\n",
      "()\n",
      "MSE = 157.928279419\n",
      "RMSE = 12.5669518746\n"
     ]
    }
   ],
   "source": [
    "Y_test[Y_test > R_early] = R_early                     # R_early = 125 \n",
    "preds = LSTM_2d.evaluate(x = X_test, y = Y_test)\n",
    "print()\n",
    "print (\"MSE = \" + str(preds))\n",
    "print (\"RMSE = \" + str(np.sqrt(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x     = range(0,100)\n",
    "y_ts  = np.sort(Y_test[:,0])\n",
    "idx   = np.argsort(Y_test[:,0])\n",
    "y_tr  = y_hat[idx,0]\n",
    "plt.plot(x, y_tr, 'bo-', x, y_ts, 'ro-')\n",
    "plt.title('RUL vs. engine #')\n",
    "plt.ylabel('RUL')\n",
    "plt.xlabel('Sorted engine #')\n",
    "plt.legend(['Prediction', 'Target'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_test, y_hat, 'bo')\n",
    "plt.plot(Y_test,Y_test, 'r-')\n",
    "plt.plot(Y_test,Y_test+20, 'r--')\n",
    "plt.plot(Y_test,Y_test-20, 'r--')\n",
    "plt.title('RUL vs. RUL #')\n",
    "plt.ylabel('RUL Estimated')\n",
    "plt.xlabel('RUL True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = y_hat - Y_test\n",
    "plt.hist(d, bins='auto')  \n",
    "plt.title('Error distribution - Test Set')\n",
    "plt.ylabel('f')\n",
    "plt.xlabel(\"Error: $RUL_{hat}$ - RUL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_train, bins=20)  \n",
    "plt.title('RUL distribution - Training Set')\n",
    "plt.ylabel('f')\n",
    "plt.xlabel(\"RUL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_test, bins=20)  \n",
    "plt.title('RUL distribution - Test Set')\n",
    "plt.ylabel('f')\n",
    "plt.xlabel(\"RUL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_hat, bins=20)  \n",
    "plt.title('RUL distribution - Test Set')\n",
    "plt.ylabel('f')\n",
    "plt.xlabel(\"RUL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
